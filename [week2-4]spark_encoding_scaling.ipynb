{"cells":[{"cell_type":"markdown","source":["### Label Encoding 수행\n* StringIndexer 객체를 이용하여 Label Encoding 적용\n* StringIndexer 객체 생성 시 변환될 컬럼명과 변환 후 컬럼명을 입력 받음.\n* StringIndexer 객체의 fit()메소드 호출 시 DataFrame 입력하면 StringInxerModel이 반환됨.\n* 반환된 StringInxerModel 객체의 transform() 메소드 호출시 DataFrame 입력하면 Label Encoding 적용된 outputCol이 추가된 DataFrame반환."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0bb49124-e423-4ada-8967-07f0e070beb8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n    [\"id\", \"category\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7d6fe57f-3dfa-4202-9b23-55ed809b95bb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+\n| id|category|\n+---+--------+\n|  0|       a|\n|  1|       b|\n|  2|       c|\n|  3|       a|\n|  4|       a|\n|  5|       c|\n+---+--------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\n# StringIndexer 클래스의 생성 인자로 DataFrame에서 Label 변환이 될 컬럼명인 inputCol, 그리고 변환 결과 컬럼명인 outputCol 필요)\nindexer = StringIndexer(inputCol='category', outputCol='category_index') # StringIndexer의 인스턴스 생성\n\n# StringIndexer는 fit() 수행시 DataFrame을 입력 받고, StringIndexerModel 객체를 반환함. \nindexer_model = indexer.fit(df) # Model 객체를 반환\nprint(indexer_model)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"329e7ff8-333c-45dc-a495-4b0f0c474f64","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["StringIndexerModel: uid=StringIndexer_dcd275010a37, handleInvalid=error\n"]}],"execution_count":0},{"cell_type":"code","source":["# StringIndexerModel에 transform()을 적용하여 outputCol로 지정된 컬럼명으로 Label Encoding 적용한 DataFrame 생성 반환. \nindexed_df = indexer_model.transform(df)\ndisplay(indexed_df.show())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e58f1004-a860-41b8-b6af-4543a6a16dcf","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+--------------+\n| id|category|category_index|\n+---+--------+--------------+\n|  0|       a|           0.0|\n|  1|       b|           2.0|\n|  2|       c|           1.0|\n|  3|       a|           0.0|\n|  4|       a|           0.0|\n|  5|       c|           1.0|\n+---+--------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### IndexToString 클래스를 이용하여 Label Encoding된 값을 원본 값으로 원복 할 수 있음."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f5fa6b47-60de-477e-985f-36c738d6b12e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import IndexToString\n\nconverter = IndexToString(inputCol='category_index', outputCol='original_category')\nconverted = converter.transform(indexed_df) # 얘는 모델 없이 transform만 있음\nconverted.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4577f56d-c226-43e3-a272-c229475d37e7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+--------+--------------+-----------------+\n| id|category|category_index|original_category|\n+---+--------+--------------+-----------------+\n|  0|       a|           0.0|                a|\n|  1|       b|           2.0|                b|\n|  2|       c|           1.0|                c|\n|  3|       a|           0.0|                a|\n|  4|       a|           0.0|                a|\n|  5|       c|           1.0|                c|\n+---+--------+--------------+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### 여러개의 컬럼을 Label Encoding 수행. \n* StringIndexer 객체 생성 시 inputCols에 리스트로 변환될 컬럼들을 입력하고, outputCols에 새롭게 변환된 컬럼명을 입력"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c2d3cc48-6bcb-40cb-acb2-c48b629901cd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n\ndf = spark.createDataFrame(\n    [(0, \"a\", \"A\"), (1, \"b\", \"A\"), (2, \"c\", \"K\"), (3, \"a\", \"D\"), (4, \"a\", \"C\"), (5, \"c\", \"B\")],\n    [\"id\", \"category1\", \"category2\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0e9c174f-6563-403b-bf0a-9e17ea6629cd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------+---------+\n| id|category1|category2|\n+---+---------+---------+\n|  0|        a|        A|\n|  1|        b|        A|\n|  2|        c|        K|\n|  3|        a|        D|\n|  4|        a|        C|\n|  5|        c|        B|\n+---+---------+---------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["indexer = StringIndexer(inputCols=[\"category1\", \"category2\"], outputCols=[\"label_encoded1\", \"label_encoded2\"])\nindexed_model = indexer.fit(df)\nindexed_df = indexed_model.transform(df)\nindexed_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"662aa03f-eb9a-4f2a-aac4-d0a39c7711bd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------+---------+--------------+--------------+\n| id|category1|category2|label_encoded1|label_encoded2|\n+---+---------+---------+--------------+--------------+\n|  0|        a|        A|           0.0|           0.0|\n|  1|        b|        A|           2.0|           0.0|\n|  2|        c|        K|           1.0|           4.0|\n|  3|        a|        D|           0.0|           3.0|\n|  4|        a|        C|           0.0|           2.0|\n|  5|        c|        B|           1.0|           1.0|\n+---+---------+---------+--------------+--------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### One Hot Encoding 적용\n* OneHotEncoder 클래스를 이용하여 변환\n* OneHotEncoder될 컬럼은 반드시 숫자형으로 변환되어 있어야 함. 따라서 OneHotEncoder를 String 컬럼에 적용 시에는 Label Encoding을 먼저 적용 후에 변환해야 함."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d60c33cd-09b0-444c-b582-75c820edbc4e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n\ndf = spark.createDataFrame([\n    (0.0, 1.0),\n    (1.0, 0.0),\n    (2.0, 1.0),\n    (0.0, 2.0),\n    (0.0, 1.0),\n    (2.0, 0.0)\n], [\"categoryIndex1\", \"categoryIndex2\"])\n\n# dropLast는 마지막 인자를 제외할지를 나타냄 default는 True. \n# 5개의 카테고리(0, 1, 2, 3, 4)가 있을 경우 2는 [0.0, 0.0, 1.0, 0.0] 로 매핑. 4는 [0.0, 0.0, 0.0, 0.0]로 매핑. \nencoder = OneHotEncoder(dropLast=True, inputCols=[\"categoryIndex1\", \"categoryIndex2\"],\n                        outputCols=[\"onehot_encoded1\", \"onehot_encoded2\"])\nencoded_model = encoder.fit(df)\n# OneHotEncoder는 sparse vector 형태로 onehot encoding 적용. \nencoded_df = encoded_model.transform(df)\n#encoded_df = encoded_model.fit(df).transform(df) # scitkit의 fit_transform()과 같이 좀 더 축약된 코드\n\nprint(encoded_df.show())\ndisplay(encoded_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e0c924c-ce39-4873-b107-5bdd011014ab","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------+--------------+---------------+---------------+\n|categoryIndex1|categoryIndex2|onehot_encoded1|onehot_encoded2|\n+--------------+--------------+---------------+---------------+\n|           0.0|           1.0|  (2,[0],[1.0])|  (2,[1],[1.0])|\n|           1.0|           0.0|  (2,[1],[1.0])|  (2,[0],[1.0])|\n|           2.0|           1.0|      (2,[],[])|  (2,[1],[1.0])|\n|           0.0|           2.0|  (2,[0],[1.0])|      (2,[],[])|\n|           0.0|           1.0|  (2,[0],[1.0])|  (2,[1],[1.0])|\n|           2.0|           0.0|      (2,[],[])|  (2,[0],[1.0])|\n+--------------+--------------+---------------+---------------+\n\nNone\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0.0,1.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]}],[1.0,0.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]}],[2.0,1.0,{"vectorType":"sparse","length":2,"indices":[],"values":[]},{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]}],[0.0,2.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":2,"indices":[],"values":[]}],[0.0,1.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]}],[2.0,0.0,{"vectorType":"sparse","length":2,"indices":[],"values":[]},{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"categoryIndex1","type":"\"double\"","metadata":"{}"},{"name":"categoryIndex2","type":"\"double\"","metadata":"{}"},{"name":"onehot_encoded1","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"0\"},{\"idx\":1,\"name\":\"1\"}]},\"num_attrs\":2}}"},{"name":"onehot_encoded2","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"0\"},{\"idx\":1,\"name\":\"1\"}]},\"num_attrs\":2}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>categoryIndex1</th><th>categoryIndex2</th><th>onehot_encoded1</th><th>onehot_encoded2</th></tr></thead><tbody><tr><td>0.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td></tr><tr><td>1.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td></tr><tr><td>2.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(), values -> List())</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td></tr><tr><td>0.0</td><td>2.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(), values -> List())</td></tr><tr><td>0.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td></tr><tr><td>2.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(), values -> List())</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(0, \"a\", \"A\"), (1, \"b\", \"A\"), (2, \"c\", \"K\"), (3, \"a\", \"D\"), (4, \"a\", \"C\"), (5, \"c\", \"B\")],\n    [\"id\", \"category1\", \"category2\"])\nprint(df.show())\n\nencoder = OneHotEncoder(inputCols=[\"category1\", \"category2\"],\n                        outputCols=[\"onehot_encoded1\", \"onehot_encoded2\"])\n                        \n# 아래 코드는 string값을 One Hot Encoding 적용 시도 하였기에 오류 발생. \nencoded_model = encoder.fit(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"828f7013-e6ce-4408-91e7-ec956ac91fdd","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+---------+---------+\n| id|category1|category2|\n+---+---------+---------+\n|  0|        a|        A|\n|  1|        b|        A|\n|  2|        c|        K|\n|  3|        a|        D|\n|  4|        a|        C|\n|  5|        c|        B|\n+---+---------+---------+\n\nNone\n"]},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-1916978023792376>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;31m# 아래 코드는 string값을 One Hot Encoding 적용 시도 하였기에 오류 발생.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mencoded_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\n\u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py\u001B[0m in \u001B[0;36mpatched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m             \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m                 \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36msafe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    523\u001B[0m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    524\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 525\u001B[0;31m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    526\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    527\u001B[0m                     \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"succeeded\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mpatch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n\u001B[1;32m    240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    241\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 242\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    243\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mException\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m                 \u001B[0;31m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mpatched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    923\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_log\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    924\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0m_AUTOLOGGING_METRICS_MANAGER\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisable_log_post_training_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 925\u001B[0;31m                     \u001B[0mfit_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfit_mlflow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    926\u001B[0m                 \u001B[0;31m# In some cases the `fit_result` may be an iterator of spark models.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    927\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mshould_log_post_training_metrics\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfit_result\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mfit_mlflow\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    900\u001B[0m         \u001B[0;31m# Note that featurizers will be autologged when they're fit as part of a Pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    901\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_get_fully_qualified_class_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pyspark.ml.feature.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 902\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    903\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    904\u001B[0m             \u001B[0;31m# skip the case params is a list or tuple, this case it will call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mcall_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n\u001B[1;32m    479\u001B[0m                                 \u001B[0mdisable_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreroute_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    480\u001B[0m                             ):\n\u001B[0;32m--> 481\u001B[0;31m                                 \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mog_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mog_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    482\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    483\u001B[0m                             try_log_autologging_event(\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    163\u001B[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 335\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    336\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    337\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    330\u001B[0m         \"\"\"\n\u001B[1;32m    331\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 332\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column category1 must be of type numeric but was actually of type string.","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Column category1 must be of type numeric but was actually of type string.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n","\u001B[0;32m<command-1916978023792376>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n","\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      9\u001B[0m \u001B[0;31m# 아래 코드는 string값을 One Hot Encoding 적용 시도 하였기에 오류 발생.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m---> 10\u001B[0;31m \u001B[0mencoded_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mencoder\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\n","\u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py\u001B[0m in \u001B[0;36mpatched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n","\u001B[1;32m     28\u001B[0m             \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m     29\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m---> 30\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m     31\u001B[0m                 \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m     32\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36msafe_patch_function\u001B[0;34m(*args, **kwargs)\u001B[0m\n","\u001B[1;32m    523\u001B[0m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    524\u001B[0m                     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 525\u001B[0;31m                         \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcall_original\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    526\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    527\u001B[0m                     \u001B[0msession\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstate\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"succeeded\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mpatch_with_managed_run\u001B[0;34m(original, *args, **kwargs)\u001B[0m\n","\u001B[1;32m    240\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    241\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 242\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpatch_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    243\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mException\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mKeyboardInterrupt\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    244\u001B[0m                 \u001B[0;31m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mpatched_fit\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n","\u001B[1;32m    923\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshould_log\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    924\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0m_AUTOLOGGING_METRICS_MANAGER\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdisable_log_post_training_metrics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 925\u001B[0;31m                     \u001B[0mfit_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfit_mlflow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0moriginal\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    926\u001B[0m                 \u001B[0;31m# In some cases the `fit_result` may be an iterator of spark models.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    927\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mshould_log_post_training_metrics\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfit_result\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/pyspark/ml/__init__.py\u001B[0m in \u001B[0;36mfit_mlflow\u001B[0;34m(original, self, *args, **kwargs)\u001B[0m\n","\u001B[1;32m    900\u001B[0m         \u001B[0;31m# Note that featurizers will be autologged when they're fit as part of a Pipeline.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    901\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0m_get_fully_qualified_class_name\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pyspark.ml.feature.\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 902\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    903\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    904\u001B[0m             \u001B[0;31m# skip the case params is a list or tuple, this case it will call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python/lib/python3.8/site-packages/mlflow/utils/autologging_utils/safety.py\u001B[0m in \u001B[0;36mcall_original\u001B[0;34m(*og_args, **og_kwargs)\u001B[0m\n","\u001B[1;32m    479\u001B[0m                                 \u001B[0mdisable_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreroute_warnings\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    480\u001B[0m                             ):\n","\u001B[0;32m--> 481\u001B[0;31m                                 \u001B[0moriginal_result\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mog_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mog_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    482\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    483\u001B[0m                             try_log_autologging_event(\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n","\u001B[1;32m    159\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    160\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 161\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    162\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    163\u001B[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n","\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 335\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    336\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    337\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n","\u001B[1;32m    330\u001B[0m         \"\"\"\n","\u001B[1;32m    331\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 332\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    333\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    334\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.1-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1302\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1304\u001B[0;31m         return_value = get_return_value(\n","\u001B[0m\u001B[1;32m   1305\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    121\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    122\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 123\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    124\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    125\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column category1 must be of type numeric but was actually of type string."]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.createDataFrame(\n    [(0, \"a\", \"A\"), (1, \"b\", \"A\"), (2, \"c\", \"K\"), (3, \"a\", \"D\"), (4, \"a\", \"C\"), (5, \"c\", \"B\")],\n    [\"id\", \"category1\", \"category2\"])\n\n# StringIndexer를 이용하여 label encoding 적용. \nlabel_encoder = StringIndexer(inputCols=[\"category1\", \"category2\"], outputCols=[\"label_encoded1\", \"label_encoded2\"])\nlabel_encoded_df = label_encoder.fit(df).transform(df) # label_encoder.fit(df) : 모델에 트랜스폼\n\n# 앞에서 숫자로 변환된 label encoding 컬럼들을 One Hot encoding 적용. \nonehot_encoder = OneHotEncoder(inputCols=[\"label_encoded1\", \"label_encoded2\"],\n                        outputCols=[\"onehot_encoded1\", \"onehot_encoded2\"])\n                        \n# 앞에서 Label encoding 변환된 DataFrame을 이용해서 One Hot encoding 적용해야함\nonehot_encoded_df = onehot_encoder.fit(label_encoded_df).transform(label_encoded_df)\n\ndisplay(onehot_encoded_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f40dfd16-dc8f-429d-a0cc-91a3b7b11759","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,"a","A",0.0,0.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[1,"b","A",2.0,0.0,{"vectorType":"sparse","length":2,"indices":[],"values":[]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[2,"c","K",1.0,4.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[],"values":[]}],[3,"a","D",0.0,3.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[3],"values":[1.0]}],[4,"a","C",0.0,2.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[2],"values":[1.0]}],[5,"c","B",1.0,1.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[1],"values":[1.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"category1","type":"\"string\"","metadata":"{}"},{"name":"category2","type":"\"string\"","metadata":"{}"},{"name":"label_encoded1","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"label_encoded1\"}}"},{"name":"label_encoded2","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"A\",\"B\",\"C\",\"D\",\"K\"],\"type\":\"nominal\",\"name\":\"label_encoded2\"}}"},{"name":"onehot_encoded1","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"a\"},{\"idx\":1,\"name\":\"c\"}]},\"num_attrs\":2}}"},{"name":"onehot_encoded2","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"A\"},{\"idx\":1,\"name\":\"B\"},{\"idx\":2,\"name\":\"C\"},{\"idx\":3,\"name\":\"D\"}]},\"num_attrs\":4}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>category1</th><th>category2</th><th>label_encoded1</th><th>label_encoded2</th><th>onehot_encoded1</th><th>onehot_encoded2</th></tr></thead><tbody><tr><td>0</td><td>a</td><td>A</td><td>0.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>1</td><td>b</td><td>A</td><td>2.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(), values -> List())</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>2</td><td>c</td><td>K</td><td>1.0</td><td>4.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(), values -> List())</td></tr><tr><td>3</td><td>a</td><td>D</td><td>0.0</td><td>3.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(3), values -> List(1.0))</td></tr><tr><td>4</td><td>a</td><td>C</td><td>0.0</td><td>2.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(2), values -> List(1.0))</td></tr><tr><td>5</td><td>c</td><td>B</td><td>1.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(1), values -> List(1.0))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Pipeline을 이용하여 OneHot Encoding 적용 \n* StringIndexer 객체와 OneHotEncoder 객체를 각각 stage로 Pipeline에 등록하여 encoding 변환."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b752cb45-7d20-4919-8ba2-1fa37249a7fc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\n# Stage로 사용될 StringIndexer 객체와 OneHotEncoder 객체 생성. \nstage_1 = StringIndexer(inputCols=['category1', 'category2'], outputCols=['label_encoded1', 'label_encoded2'])\nstage_2 = OneHotEncoder(inputCols=['label_encoded1', 'label_encoded2'], outputCols=['onehot_encoded1', 'onehot_encoded2'])\n\n# stage로 StringIndexer객체와 OneHotEncoder 객체 등록하여 Pipeline 객체 생성. \npipeline = Pipeline(stages=[stage_1, stage_2])\n\n# pipeline.fit(df) 수행하여 PipelineModel 생성하고 PipelineModel의 transfrom(df) 호출하여 최종 변환. \npipeline_model = pipeline.fit(df)\nonehot_encoded_df = pipeline_model.transform(df)\n#onehot_encoded_df = pipeline.fit(df).transform(df)\n\ndisplay(onehot_encoded_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"28b94b72-543d-4c0b-a5c1-e0e677f5e66b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0,"a","A",0.0,0.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[1,"b","A",2.0,0.0,{"vectorType":"sparse","length":2,"indices":[],"values":[]},{"vectorType":"sparse","length":4,"indices":[0],"values":[1.0]}],[2,"c","K",1.0,4.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[],"values":[]}],[3,"a","D",0.0,3.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[3],"values":[1.0]}],[4,"a","C",0.0,2.0,{"vectorType":"sparse","length":2,"indices":[0],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[2],"values":[1.0]}],[5,"c","B",1.0,1.0,{"vectorType":"sparse","length":2,"indices":[1],"values":[1.0]},{"vectorType":"sparse","length":4,"indices":[1],"values":[1.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"id","type":"\"long\"","metadata":"{}"},{"name":"category1","type":"\"string\"","metadata":"{}"},{"name":"category2","type":"\"string\"","metadata":"{}"},{"name":"label_encoded1","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"a\",\"c\",\"b\"],\"type\":\"nominal\",\"name\":\"label_encoded1\"}}"},{"name":"label_encoded2","type":"\"double\"","metadata":"{\"ml_attr\":{\"vals\":[\"A\",\"B\",\"C\",\"D\",\"K\"],\"type\":\"nominal\",\"name\":\"label_encoded2\"}}"},{"name":"onehot_encoded1","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"a\"},{\"idx\":1,\"name\":\"c\"}]},\"num_attrs\":2}}"},{"name":"onehot_encoded2","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"binary\":[{\"idx\":0,\"name\":\"A\"},{\"idx\":1,\"name\":\"B\"},{\"idx\":2,\"name\":\"C\"},{\"idx\":3,\"name\":\"D\"}]},\"num_attrs\":4}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>category1</th><th>category2</th><th>label_encoded1</th><th>label_encoded2</th><th>onehot_encoded1</th><th>onehot_encoded2</th></tr></thead><tbody><tr><td>0</td><td>a</td><td>A</td><td>0.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>1</td><td>b</td><td>A</td><td>2.0</td><td>0.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(), values -> List())</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(0), values -> List(1.0))</td></tr><tr><td>2</td><td>c</td><td>K</td><td>1.0</td><td>4.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(), values -> List())</td></tr><tr><td>3</td><td>a</td><td>D</td><td>0.0</td><td>3.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(3), values -> List(1.0))</td></tr><tr><td>4</td><td>a</td><td>C</td><td>0.0</td><td>2.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(0), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(2), values -> List(1.0))</td></tr><tr><td>5</td><td>c</td><td>B</td><td>1.0</td><td>1.0</td><td>Map(vectorType -> sparse, length -> 2, indices -> List(1), values -> List(1.0))</td><td>Map(vectorType -> sparse, length -> 4, indices -> List(1), values -> List(1.0))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["print(pipeline_model.stages)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"816c208a-0e28-45d9-a1ec-08a03643332d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[StringIndexerModel: uid=StringIndexer_6cd34f0c9ef8, handleInvalid=error, numInputCols=2, numOutputCols=2, OneHotEncoderModel: uid=OneHotEncoder_1b8a0d970f7b, dropLast=true, handleInvalid=error, numInputCols=2, numOutputCols=2]\n"]}],"execution_count":0},{"cell_type":"markdown","source":["### Scaling의 적용\n* Standard 스케일링은 StandardScaler 클래스로, Min Max 스케일링은 MinMaxClass를 이용하여 적용. \n* 주의할 사용한 Scaling은 일반 컬럼형(숫자형)이 아니라 vector형에만 적용이 가능함. 이는 Spark ML이 통계 전용의 기능을 제공하기 보다는 ML에 주로 특화 되었기 때문\n* 때문에 단일 컬럼에 Scaling을 적용할 때도 반드시 VectorAssembler로 변환 후에 적용해야 함"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2f566ef0-4d96-4df7-9852-f46a15195989","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\nimport numpy as np\nimport pandas as pd\n\n# iris 데이터 세트 로딩하고  iris 데이터 세트를 numpy에서 pandas DataFrame으로 변환 \niris = load_iris()\niris_data = iris.data\niris_label = iris.target\n\niris_columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\niris_pdf = pd.DataFrame(iris_data, columns=iris_columns)\niris_pdf['label'] = iris_label\n\n# pandas DataFrame을 spark DataFrame으로 변환\niris_sdf = spark.createDataFrame(iris_pdf)\n\ndisplay(iris_sdf.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d1d7319-a910-4ba5-aaf4-81019e95ef33","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{},"output_type":"display_data","data":{"text/plain":[],"application/vnd.databricks.v1+bamboolib_hint":"{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}"}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[5.1,3.5,1.4,0.2,0],[4.9,3.0,1.4,0.2,0],[4.7,3.2,1.3,0.2,0],[4.6,3.1,1.5,0.2,0],[5.0,3.6,1.4,0.2,0],[5.4,3.9,1.7,0.4,0],[4.6,3.4,1.4,0.3,0],[5.0,3.4,1.5,0.2,0],[4.4,2.9,1.4,0.2,0],[4.9,3.1,1.5,0.1,0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sepal_length","type":"\"double\"","metadata":"{}"},{"name":"sepal_width","type":"\"double\"","metadata":"{}"},{"name":"petal_length","type":"\"double\"","metadata":"{}"},{"name":"petal_width","type":"\"double\"","metadata":"{}"},{"name":"label","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>label</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0</td></tr><tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0</td></tr><tr><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0</td></tr><tr><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0</td></tr><tr><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0</td></tr><tr><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler\n\n# number type 단일 컬럼에 StandardScaler를 적용하면 오류 발생. Vector 형으로 해당 컬럼을 변경해야 함. \nstandard_scaler = StandardScaler(inputCol='sepal_length', outputCol='scaled_sepal_length')\nstandard_scaler_model = standard_scaler.fit(iris_sdf)\nstandard_scaled_df = standard_scaler_model.transform(iris_sdf)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"50a96e37-143d-4ab5-aa57-023352ecccf9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n\u001B[0;32m<command-328170807580035>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;31m# number type 단일 컬럼에 StandardScaler를 적용하면 오류 발생. Vector 형으로 해당 컬럼을 변경해야 함.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mstandard_scaler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStandardScaler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'sepal_length'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'scaled_sepal_length'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mstandard_scaler_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstandard_scaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miris_sdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mstandard_scaled_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstandard_scaler_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miris_sdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py\u001B[0m in \u001B[0;36mpatched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     28\u001B[0m             \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     29\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 30\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     31\u001B[0m                 \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    203\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 205\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    207\u001B[0m             raise TypeError(\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 383\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    384\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    385\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    378\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    379\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 380\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column sepal_length must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.DoubleType$:double.","errorSummary":"<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Column sepal_length must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.DoubleType$:double.","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n","\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n","\u001B[0;32m<command-328170807580035>\u001B[0m in \u001B[0;36m<cell line: 5>\u001B[0;34m()\u001B[0m\n","\u001B[1;32m      3\u001B[0m \u001B[0;31m# number type 단일 컬럼에 StandardScaler를 적용하면 오류 발생. Vector 형으로 해당 컬럼을 변경해야 함.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m      4\u001B[0m \u001B[0mstandard_scaler\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mStandardScaler\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'sepal_length'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputCol\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'scaled_sepal_length'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m----> 5\u001B[0;31m \u001B[0mstandard_scaler_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstandard_scaler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miris_sdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mstandard_scaled_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstandard_scaler_model\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miris_sdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/python_shell/dbruntime/MLWorkloadsInstrumentation/_pyspark.py\u001B[0m in \u001B[0;36mpatched_method\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n","\u001B[1;32m     28\u001B[0m             \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m     29\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m---> 30\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moriginal_method\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m     31\u001B[0m                 \u001B[0mcall_succeeded\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m     32\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, dataset, params)\u001B[0m\n","\u001B[1;32m    203\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    204\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 205\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    206\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    207\u001B[0m             raise TypeError(\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit\u001B[0;34m(self, dataset)\u001B[0m\n","\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 383\u001B[0;31m         \u001B[0mjava_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fit_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    384\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_create_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjava_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    385\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_copyValues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_fit_java\u001B[0;34m(self, dataset)\u001B[0m\n","\u001B[1;32m    378\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    379\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 380\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    381\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    382\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_fit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mJM\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n","\u001B[1;32m   1319\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m   1320\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m-> 1321\u001B[0;31m         return_value = get_return_value(\n","\u001B[0m\u001B[1;32m   1322\u001B[0m             answer, self.gateway_client, self.target_id, self.name)\n","\u001B[1;32m   1323\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;32m/databricks/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n","\u001B[1;32m    200\u001B[0m                 \u001B[0;31m# Hide where the exception came from that shows a non-Pythonic\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    201\u001B[0m                 \u001B[0;31m# JVM exception message.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m--> 202\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mconverted\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0m\u001B[1;32m    203\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[1;32m    204\u001B[0m                 \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\n","\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Column sepal_length must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.sql.types.DoubleType$:double."]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StandardScaler\n\n# VectorAssembler는 반드시 생성자로 inputCols를 list 형으로 받아야 함. inputCol은 안됨.  \n# vec_assembler = VectorAssembler(inputCol=sepal_length, outputCol='sepal_length_vector')는 오류 발생. \nvec_assembler = VectorAssembler(inputCols=['sepal_length'], outputCol='sepal_length_vector')\n\n# VectorAssembler는 fit()이 없음. \niris_sdf_vectorized = vec_assembler.transform(iris_sdf)\n\ndisplay(iris_sdf_vectorized.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc2dbb04-110e-4d1e-b10d-34cf7a84a303","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[5.1,3.5,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[5.1]}],[4.9,3.0,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[4.9]}],[4.7,3.2,1.3,0.2,0,{"vectorType":"dense","length":1,"values":[4.7]}],[4.6,3.1,1.5,0.2,0,{"vectorType":"dense","length":1,"values":[4.6]}],[5.0,3.6,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[5.0]}],[5.4,3.9,1.7,0.4,0,{"vectorType":"dense","length":1,"values":[5.4]}],[4.6,3.4,1.4,0.3,0,{"vectorType":"dense","length":1,"values":[4.6]}],[5.0,3.4,1.5,0.2,0,{"vectorType":"dense","length":1,"values":[5.0]}],[4.4,2.9,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[4.4]}],[4.9,3.1,1.5,0.1,0,{"vectorType":"dense","length":1,"values":[4.9]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sepal_length","type":"\"double\"","metadata":"{}"},{"name":"sepal_width","type":"\"double\"","metadata":"{}"},{"name":"petal_length","type":"\"double\"","metadata":"{}"},{"name":"petal_width","type":"\"double\"","metadata":"{}"},{"name":"label","type":"\"long\"","metadata":"{}"},{"name":"sepal_length_vector","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"sepal_length\"}]},\"num_attrs\":1}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>label</th><th>sepal_length_vector</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.1))</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.9))</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.7))</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.6))</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.0))</td></tr><tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.4))</td></tr><tr><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.6))</td></tr><tr><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.0))</td></tr><tr><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.4))</td></tr><tr><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.9))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# vector화된 컬럼에 대해서 StandardScaler 적용 \nstandard_scaler = StandardScaler(inputCol='sepal_length_vector', outputCol='standard_scaled_vector_01')\nstandard_scaler_model = standard_scaler.fit(iris_sdf_vectorized)\nstandard_scaled_df = standard_scaler_model.transform(iris_sdf_vectorized)\ndisplay(standard_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f9bb1082-fa68-4884-8277-49a9cc1ca22f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[5.1,3.5,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[5.1]},{"vectorType":"dense","length":1,"values":[6.158928408838792]}],[4.9,3.0,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[4.9]},{"vectorType":"dense","length":1,"values":[5.917401804570605]}],[4.7,3.2,1.3,0.2,0,{"vectorType":"dense","length":1,"values":[4.7]},{"vectorType":"dense","length":1,"values":[5.675875200302417]}],[4.6,3.1,1.5,0.2,0,{"vectorType":"dense","length":1,"values":[4.6]},{"vectorType":"dense","length":1,"values":[5.555111898168322]}],[5.0,3.6,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[5.0]},{"vectorType":"dense","length":1,"values":[6.038165106704698]}],[5.4,3.9,1.7,0.4,0,{"vectorType":"dense","length":1,"values":[5.4]},{"vectorType":"dense","length":1,"values":[6.521218315241074]}],[4.6,3.4,1.4,0.3,0,{"vectorType":"dense","length":1,"values":[4.6]},{"vectorType":"dense","length":1,"values":[5.555111898168322]}],[5.0,3.4,1.5,0.2,0,{"vectorType":"dense","length":1,"values":[5.0]},{"vectorType":"dense","length":1,"values":[6.038165106704698]}],[4.4,2.9,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[4.4]},{"vectorType":"dense","length":1,"values":[5.313585293900135]}],[4.9,3.1,1.5,0.1,0,{"vectorType":"dense","length":1,"values":[4.9]},{"vectorType":"dense","length":1,"values":[5.917401804570605]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sepal_length","type":"\"double\"","metadata":"{}"},{"name":"sepal_width","type":"\"double\"","metadata":"{}"},{"name":"petal_length","type":"\"double\"","metadata":"{}"},{"name":"petal_width","type":"\"double\"","metadata":"{}"},{"name":"label","type":"\"long\"","metadata":"{}"},{"name":"sepal_length_vector","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"sepal_length\"}]},\"num_attrs\":1}}"},{"name":"standard_scaled_vector_01","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"num_attrs\":1}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>label</th><th>sepal_length_vector</th><th>standard_scaled_vector_01</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.1))</td><td>Map(vectorType -> dense, length -> 1, values -> List(6.158928408838792))</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.9))</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.917401804570605))</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.7))</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.675875200302417))</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.6))</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.555111898168322))</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.0))</td><td>Map(vectorType -> dense, length -> 1, values -> List(6.038165106704698))</td></tr><tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.4))</td><td>Map(vectorType -> dense, length -> 1, values -> List(6.521218315241074))</td></tr><tr><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.6))</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.555111898168322))</td></tr><tr><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.0))</td><td>Map(vectorType -> dense, length -> 1, values -> List(6.038165106704698))</td></tr><tr><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.4))</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.313585293900135))</td></tr><tr><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.9))</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.917401804570605))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["standard_scaler = StandardScaler(inputCol='sepal_length_vector', outputCol='standard_scaled_vector_02', withMean=True, withStd=True)\nstandard_scaler_model = standard_scaler.fit(iris_sdf_vectorized)\nstandard_scaled_df = standard_scaler_model.transform(iris_sdf_vectorized)\ndisplay(standard_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6c427503-5ebc-4777-9195-c938e9534ca7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[5.1,3.5,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[5.1]},{"vectorType":"dense","length":1,"values":[-0.8976738791967649]}],[4.9,3.0,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[4.9]},{"vectorType":"dense","length":1,"values":[-1.139200483464952]}],[4.7,3.2,1.3,0.2,0,{"vectorType":"dense","length":1,"values":[4.7]},{"vectorType":"dense","length":1,"values":[-1.3807270877331401]}],[4.6,3.1,1.5,0.2,0,{"vectorType":"dense","length":1,"values":[4.6]},{"vectorType":"dense","length":1,"values":[-1.5014903898672347]}],[5.0,3.6,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[5.0]},{"vectorType":"dense","length":1,"values":[-1.0184371813308586]}],[5.4,3.9,1.7,0.4,0,{"vectorType":"dense","length":1,"values":[5.4]},{"vectorType":"dense","length":1,"values":[-0.5353839727944822]}],[4.6,3.4,1.4,0.3,0,{"vectorType":"dense","length":1,"values":[4.6]},{"vectorType":"dense","length":1,"values":[-1.5014903898672347]}],[5.0,3.4,1.5,0.2,0,{"vectorType":"dense","length":1,"values":[5.0]},{"vectorType":"dense","length":1,"values":[-1.0184371813308586]}],[4.4,2.9,1.4,0.2,0,{"vectorType":"dense","length":1,"values":[4.4]},{"vectorType":"dense","length":1,"values":[-1.7430169941354219]}],[4.9,3.1,1.5,0.1,0,{"vectorType":"dense","length":1,"values":[4.9]},{"vectorType":"dense","length":1,"values":[-1.139200483464952]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sepal_length","type":"\"double\"","metadata":"{}"},{"name":"sepal_width","type":"\"double\"","metadata":"{}"},{"name":"petal_length","type":"\"double\"","metadata":"{}"},{"name":"petal_width","type":"\"double\"","metadata":"{}"},{"name":"label","type":"\"long\"","metadata":"{}"},{"name":"sepal_length_vector","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"sepal_length\"}]},\"num_attrs\":1}}"},{"name":"standard_scaled_vector_02","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"num_attrs\":1}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>label</th><th>sepal_length_vector</th><th>standard_scaled_vector_02</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.1))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-0.8976738791967649))</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.9))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.139200483464952))</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.7))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.3807270877331401))</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.6))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.5014903898672347))</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.0))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.0184371813308586))</td></tr><tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.4))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-0.5353839727944822))</td></tr><tr><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.6))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.5014903898672347))</td></tr><tr><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(5.0))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.0184371813308586))</td></tr><tr><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.4))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.7430169941354219))</td></tr><tr><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0</td><td>Map(vectorType -> dense, length -> 1, values -> List(4.9))</td><td>Map(vectorType -> dense, length -> 1, values -> List(-1.139200483464952))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# 전체 컬럼에 Standard Scaler 적용. \nvec_assembler = VectorAssembler(inputCols=iris_columns, outputCol='features')\n# 이 부분이 가장 큰 차이\nstandard_scaler = StandardScaler(inputCol='features', outputCol='standard_scaled_features', withMean=True, withStd=True)\n\niris_sdf_vectorized = vec_assembler.transform(iris_sdf)\nstandard_scaled_df = standard_scaler.fit(iris_sdf_vectorized).transform(iris_sdf_vectorized)\n\nstandard_scaled_df.limit(10).show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4deb838f-0994-42b9-8d2c-b350bbd46289","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n|sepal_length|sepal_width|petal_length|petal_width|label|features         |standard_scaled_features                                                         |\n+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n|5.1         |3.5        |1.4         |0.2        |0    |[5.1,3.5,1.4,0.2]|[-0.8976738791967649,1.0156019907136327,-1.3357516342415199,-1.3110521482051305] |\n|4.9         |3.0        |1.4         |0.2        |0    |[4.9,3.0,1.4,0.2]|[-1.139200483464952,-0.13153881205026055,-1.3357516342415199,-1.3110521482051305]|\n|4.7         |3.2        |1.3         |0.2        |0    |[4.7,3.2,1.3,0.2]|[-1.3807270877331401,0.3273175090552971,-1.3923992862449772,-1.3110521482051305] |\n|4.6         |3.1        |1.5         |0.2        |0    |[4.6,3.1,1.5,0.2]|[-1.5014903898672347,0.09788934850251829,-1.2791039822380628,-1.3110521482051305]|\n|5.0         |3.6        |1.4         |0.2        |0    |[5.0,3.6,1.4,0.2]|[-1.0184371813308586,1.2450301512664115,-1.3357516342415199,-1.3110521482051305] |\n|5.4         |3.9        |1.7         |0.4        |0    |[5.4,3.9,1.7,0.4]|[-0.5353839727944822,1.933314632924747,-1.1658086782311483,-1.0486667949952977]  |\n|4.6         |3.4        |1.4         |0.3        |0    |[4.6,3.4,1.4,0.3]|[-1.5014903898672347,0.7861738301608538,-1.3357516342415199,-1.179859471600214]  |\n|5.0         |3.4        |1.5         |0.2        |0    |[5.0,3.4,1.5,0.2]|[-1.0184371813308586,0.7861738301608538,-1.2791039822380628,-1.3110521482051305] |\n|4.4         |2.9        |1.4         |0.2        |0    |[4.4,2.9,1.4,0.2]|[-1.7430169941354219,-0.3609669726030394,-1.3357516342415199,-1.3110521482051305]|\n|4.9         |3.1        |1.5         |0.1        |0    |[4.9,3.1,1.5,0.1]|[-1.139200483464952,0.09788934850251829,-1.2791039822380628,-1.4422448248100466] |\n+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Pipeline을 이용하여 Standard Scaling  변환\nfrom pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [vec_assembler, standard_scaler])\nstandard_scaled_df = pipeline.fit(iris_sdf).transform(iris_sdf)\n\nstandard_scaled_df.limit(10).show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b71b189c-23a5-44a4-ad71-1e1ff0a9f823","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n|sepal_length|sepal_width|petal_length|petal_width|label|features         |standard_scaled_features                                                         |\n+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n|5.1         |3.5        |1.4         |0.2        |0    |[5.1,3.5,1.4,0.2]|[-0.8976738791967649,1.0156019907136327,-1.3357516342415199,-1.3110521482051305] |\n|4.9         |3.0        |1.4         |0.2        |0    |[4.9,3.0,1.4,0.2]|[-1.139200483464952,-0.13153881205026055,-1.3357516342415199,-1.3110521482051305]|\n|4.7         |3.2        |1.3         |0.2        |0    |[4.7,3.2,1.3,0.2]|[-1.3807270877331401,0.3273175090552971,-1.3923992862449772,-1.3110521482051305] |\n|4.6         |3.1        |1.5         |0.2        |0    |[4.6,3.1,1.5,0.2]|[-1.5014903898672347,0.09788934850251829,-1.2791039822380628,-1.3110521482051305]|\n|5.0         |3.6        |1.4         |0.2        |0    |[5.0,3.6,1.4,0.2]|[-1.0184371813308586,1.2450301512664115,-1.3357516342415199,-1.3110521482051305] |\n|5.4         |3.9        |1.7         |0.4        |0    |[5.4,3.9,1.7,0.4]|[-0.5353839727944822,1.933314632924747,-1.1658086782311483,-1.0486667949952977]  |\n|4.6         |3.4        |1.4         |0.3        |0    |[4.6,3.4,1.4,0.3]|[-1.5014903898672347,0.7861738301608538,-1.3357516342415199,-1.179859471600214]  |\n|5.0         |3.4        |1.5         |0.2        |0    |[5.0,3.4,1.5,0.2]|[-1.0184371813308586,0.7861738301608538,-1.2791039822380628,-1.3110521482051305] |\n|4.4         |2.9        |1.4         |0.2        |0    |[4.4,2.9,1.4,0.2]|[-1.7430169941354219,-0.3609669726030394,-1.3357516342415199,-1.3110521482051305]|\n|4.9         |3.1        |1.5         |0.1        |0    |[4.9,3.1,1.5,0.1]|[-1.139200483464952,0.09788934850251829,-1.2791039822380628,-1.4422448248100466] |\n+------------+-----------+------------+-----------+-----+-----------------+---------------------------------------------------------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["#### MinMax 스케일링 변환"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb5509cc-6099-4485-bd66-ebe2575e70d7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\n\n# 전체 feature 컬럼에 minmax scaler 적용\nvec_assembler = VectorAssembler(inputCols=iris_columns, outputCol='features')\niris_sdf_vectorized = vec_assembler.transform(iris_sdf)\n\nminmax_scaler = MinMaxScaler(inputCol='features', outputCol='minmax_scaled_features')\nminmax_scaled_df = minmax_scaler.fit(iris_sdf_vectorized).transform(iris_sdf_vectorized)\n\ndisplay(minmax_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"320b0cf3-717c-4e38-93ed-2f3e4d446173","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[5.1,3.5,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[5.1,3.5,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.22222222222222213,0.625,0.06779661016949151,0.04166666666666667]}],[4.9,3.0,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[4.9,3.0,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.1666666666666668,0.41666666666666663,0.06779661016949151,0.04166666666666667]}],[4.7,3.2,1.3,0.2,0,{"vectorType":"dense","length":4,"values":[4.7,3.2,1.3,0.2]},{"vectorType":"dense","length":4,"values":[0.11111111111111119,0.5,0.05084745762711865,0.04166666666666667]}],[4.6,3.1,1.5,0.2,0,{"vectorType":"dense","length":4,"values":[4.6,3.1,1.5,0.2]},{"vectorType":"dense","length":4,"values":[0.08333333333333327,0.4583333333333333,0.0847457627118644,0.04166666666666667]}],[5.0,3.6,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[5.0,3.6,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.19444444444444448,0.6666666666666666,0.06779661016949151,0.04166666666666667]}],[5.4,3.9,1.7,0.4,0,{"vectorType":"dense","length":4,"values":[5.4,3.9,1.7,0.4]},{"vectorType":"dense","length":4,"values":[0.30555555555555564,0.7916666666666665,0.11864406779661016,0.12500000000000003]}],[4.6,3.4,1.4,0.3,0,{"vectorType":"dense","length":4,"values":[4.6,3.4,1.4,0.3]},{"vectorType":"dense","length":4,"values":[0.08333333333333327,0.5833333333333333,0.06779661016949151,0.08333333333333333]}],[5.0,3.4,1.5,0.2,0,{"vectorType":"dense","length":4,"values":[5.0,3.4,1.5,0.2]},{"vectorType":"dense","length":4,"values":[0.19444444444444448,0.5833333333333333,0.0847457627118644,0.04166666666666667]}],[4.4,2.9,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[4.4,2.9,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.027777777777777922,0.37499999999999994,0.06779661016949151,0.04166666666666667]}],[4.9,3.1,1.5,0.1,0,{"vectorType":"dense","length":4,"values":[4.9,3.1,1.5,0.1]},{"vectorType":"dense","length":4,"values":[0.1666666666666668,0.4583333333333333,0.0847457627118644,0.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sepal_length","type":"\"double\"","metadata":"{}"},{"name":"sepal_width","type":"\"double\"","metadata":"{}"},{"name":"petal_length","type":"\"double\"","metadata":"{}"},{"name":"petal_width","type":"\"double\"","metadata":"{}"},{"name":"label","type":"\"long\"","metadata":"{}"},{"name":"features","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"sepal_length\"},{\"idx\":1,\"name\":\"sepal_width\"},{\"idx\":2,\"name\":\"petal_length\"},{\"idx\":3,\"name\":\"petal_width\"}]},\"num_attrs\":4}}"},{"name":"minmax_scaled_features","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"num_attrs\":4}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>label</th><th>features</th><th>minmax_scaled_features</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.1, 3.5, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.22222222222222213, 0.625, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.9, 3.0, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.1666666666666668, 0.41666666666666663, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.7, 3.2, 1.3, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.11111111111111119, 0.5, 0.05084745762711865, 0.04166666666666667))</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.6, 3.1, 1.5, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.08333333333333327, 0.4583333333333333, 0.0847457627118644, 0.04166666666666667))</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.0, 3.6, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.19444444444444448, 0.6666666666666666, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.4, 3.9, 1.7, 0.4))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.30555555555555564, 0.7916666666666665, 0.11864406779661016, 0.12500000000000003))</td></tr><tr><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.6, 3.4, 1.4, 0.3))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.08333333333333327, 0.5833333333333333, 0.06779661016949151, 0.08333333333333333))</td></tr><tr><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.0, 3.4, 1.5, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.19444444444444448, 0.5833333333333333, 0.0847457627118644, 0.04166666666666667))</td></tr><tr><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.4, 2.9, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.027777777777777922, 0.37499999999999994, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.9, 3.1, 1.5, 0.1))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.1666666666666668, 0.4583333333333333, 0.0847457627118644, 0.0))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n\npipeline = Pipeline(stages = [vec_assembler, minmax_scaler])\nminmax_scaled_df = pipeline.fit(iris_sdf).transform(iris_sdf)\n\ndisplay(minmax_scaled_df.limit(10))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a970ad9a-42ab-466b-aebd-8a87ca3c9716","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[5.1,3.5,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[5.1,3.5,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.22222222222222213,0.625,0.06779661016949151,0.04166666666666667]}],[4.9,3.0,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[4.9,3.0,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.1666666666666668,0.41666666666666663,0.06779661016949151,0.04166666666666667]}],[4.7,3.2,1.3,0.2,0,{"vectorType":"dense","length":4,"values":[4.7,3.2,1.3,0.2]},{"vectorType":"dense","length":4,"values":[0.11111111111111119,0.5,0.05084745762711865,0.04166666666666667]}],[4.6,3.1,1.5,0.2,0,{"vectorType":"dense","length":4,"values":[4.6,3.1,1.5,0.2]},{"vectorType":"dense","length":4,"values":[0.08333333333333327,0.4583333333333333,0.0847457627118644,0.04166666666666667]}],[5.0,3.6,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[5.0,3.6,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.19444444444444448,0.6666666666666666,0.06779661016949151,0.04166666666666667]}],[5.4,3.9,1.7,0.4,0,{"vectorType":"dense","length":4,"values":[5.4,3.9,1.7,0.4]},{"vectorType":"dense","length":4,"values":[0.30555555555555564,0.7916666666666665,0.11864406779661016,0.12500000000000003]}],[4.6,3.4,1.4,0.3,0,{"vectorType":"dense","length":4,"values":[4.6,3.4,1.4,0.3]},{"vectorType":"dense","length":4,"values":[0.08333333333333327,0.5833333333333333,0.06779661016949151,0.08333333333333333]}],[5.0,3.4,1.5,0.2,0,{"vectorType":"dense","length":4,"values":[5.0,3.4,1.5,0.2]},{"vectorType":"dense","length":4,"values":[0.19444444444444448,0.5833333333333333,0.0847457627118644,0.04166666666666667]}],[4.4,2.9,1.4,0.2,0,{"vectorType":"dense","length":4,"values":[4.4,2.9,1.4,0.2]},{"vectorType":"dense","length":4,"values":[0.027777777777777922,0.37499999999999994,0.06779661016949151,0.04166666666666667]}],[4.9,3.1,1.5,0.1,0,{"vectorType":"dense","length":4,"values":[4.9,3.1,1.5,0.1]},{"vectorType":"dense","length":4,"values":[0.1666666666666668,0.4583333333333333,0.0847457627118644,0.0]}]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"sepal_length","type":"\"double\"","metadata":"{}"},{"name":"sepal_width","type":"\"double\"","metadata":"{}"},{"name":"petal_length","type":"\"double\"","metadata":"{}"},{"name":"petal_width","type":"\"double\"","metadata":"{}"},{"name":"label","type":"\"long\"","metadata":"{}"},{"name":"features","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"attrs\":{\"numeric\":[{\"idx\":0,\"name\":\"sepal_length\"},{\"idx\":1,\"name\":\"sepal_width\"},{\"idx\":2,\"name\":\"petal_length\"},{\"idx\":3,\"name\":\"petal_width\"}]},\"num_attrs\":4}}"},{"name":"minmax_scaled_features","type":"{\"type\":\"udt\",\"class\":\"org.apache.spark.ml.linalg.VectorUDT\",\"pyClass\":\"pyspark.ml.linalg.VectorUDT\",\"sqlType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"type\",\"type\":\"byte\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"indices\",\"type\":{\"type\":\"array\",\"elementType\":\"integer\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}},{\"name\":\"values\",\"type\":{\"type\":\"array\",\"elementType\":\"double\",\"containsNull\":false},\"nullable\":true,\"metadata\":{}}]}}","metadata":"{\"ml_attr\":{\"num_attrs\":4}}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>sepal_length</th><th>sepal_width</th><th>petal_length</th><th>petal_width</th><th>label</th><th>features</th><th>minmax_scaled_features</th></tr></thead><tbody><tr><td>5.1</td><td>3.5</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.1, 3.5, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.22222222222222213, 0.625, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>4.9</td><td>3.0</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.9, 3.0, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.1666666666666668, 0.41666666666666663, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>4.7</td><td>3.2</td><td>1.3</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.7, 3.2, 1.3, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.11111111111111119, 0.5, 0.05084745762711865, 0.04166666666666667))</td></tr><tr><td>4.6</td><td>3.1</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.6, 3.1, 1.5, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.08333333333333327, 0.4583333333333333, 0.0847457627118644, 0.04166666666666667))</td></tr><tr><td>5.0</td><td>3.6</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.0, 3.6, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.19444444444444448, 0.6666666666666666, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>5.4</td><td>3.9</td><td>1.7</td><td>0.4</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.4, 3.9, 1.7, 0.4))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.30555555555555564, 0.7916666666666665, 0.11864406779661016, 0.12500000000000003))</td></tr><tr><td>4.6</td><td>3.4</td><td>1.4</td><td>0.3</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.6, 3.4, 1.4, 0.3))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.08333333333333327, 0.5833333333333333, 0.06779661016949151, 0.08333333333333333))</td></tr><tr><td>5.0</td><td>3.4</td><td>1.5</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(5.0, 3.4, 1.5, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.19444444444444448, 0.5833333333333333, 0.0847457627118644, 0.04166666666666667))</td></tr><tr><td>4.4</td><td>2.9</td><td>1.4</td><td>0.2</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.4, 2.9, 1.4, 0.2))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.027777777777777922, 0.37499999999999994, 0.06779661016949151, 0.04166666666666667))</td></tr><tr><td>4.9</td><td>3.1</td><td>1.5</td><td>0.1</td><td>0</td><td>Map(vectorType -> dense, length -> 4, values -> List(4.9, 3.1, 1.5, 0.1))</td><td>Map(vectorType -> dense, length -> 4, values -> List(0.1666666666666668, 0.4583333333333333, 0.0847457627118644, 0.0))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54b53069-281e-42fb-9b06-b1a5e0e6d120","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"[week2-4]spark_encoding_scaling","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":328170807580016}},"nbformat":4,"nbformat_minor":0}
